{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 8: Backtesting\n",
    "\n",
    "In this project, you will build a fairly realistic backtester that uses the Barra data. The backtester will perform portfolio optimization that includes transaction costs, and you'll implement it with computational efficiency in mind, to allow for a reasonably fast backtest. You'll also use performance attribution to identify the major drivers of your portfolio's profit-and-loss (PnL). You will have the option to modify and customize the backtest as well.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n",
    "\n",
    "## Packages\n",
    "\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==2.1.0 (from -r requirements.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/90/cf10bb2020d2811da811a49601f6eafcda022c6ccd296fd05aba093dee96/matplotlib-2.1.0.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    IMPORTANT WARNING:\n",
      "        pkg-config is not installed.\n",
      "        matplotlib may not be able to find some of its dependencies\n",
      "    ============================================================================\n",
      "    Edit setup.cfg to change the build options\n",
      "    \n",
      "    BUILDING MATPLOTLIB\n",
      "                matplotlib: yes [2.1.0]\n",
      "                    python: yes [3.7.3 (default, Mar 27 2019, 16:54:48)  [Clang\n",
      "                            4.0.1 (tags/RELEASE_401/final)]]\n",
      "                  platform: yes [darwin]\n",
      "    \n",
      "    REQUIRED DEPENDENCIES AND EXTENSIONS\n",
      "                     numpy: yes [version 1.16.2]\n",
      "                       six: yes [using six version 1.12.0]\n",
      "                  dateutil: yes [using dateutil version 2.8.0]\n",
      "    backports.functools_lru_cache: yes [Not required]\n",
      "              subprocess32: yes [Not required]\n",
      "                      pytz: yes [using pytz version 2019.1]\n",
      "                    cycler: yes [using cycler version 0.10.0]\n",
      "                   tornado: yes [using tornado version 6.0.2]\n",
      "                 pyparsing: yes [using pyparsing version 2.4.0]\n",
      "                    libagg: yes [pkg-config information for 'libagg' could not\n",
      "                            be found. Using local copy.]\n",
      "                  freetype: no  [The C/C++ header for freetype2 (ft2build.h)\n",
      "                            could not be found.  You may need to install the\n",
      "                            development package.]\n",
      "                       png: yes [version 1.6.36]\n",
      "                     qhull: yes [pkg-config information for 'libqhull' could not\n",
      "                            be found. Using local copy.]\n",
      "    \n",
      "    OPTIONAL SUBPACKAGES\n",
      "               sample_data: yes [installing]\n",
      "                  toolkits: yes [installing]\n",
      "                     tests: no  [skipping due to configuration]\n",
      "            toolkits_tests: no  [skipping due to configuration]\n",
      "    \n",
      "    OPTIONAL BACKEND EXTENSIONS\n",
      "                    macosx: yes [installing, darwin]\n",
      "                    qt5agg: yes [installing, Qt: 5.9.6, PyQt: 5.9.6; PySide2 not\n",
      "                            found]\n",
      "                    qt4agg: no  [PySide not found; PyQt4 not found]\n",
      "                   gtk3agg: no  [Requires pygobject to be installed.]\n",
      "                 gtk3cairo: no  [Requires cairocffi or pycairo to be installed.]\n",
      "                    gtkagg: no  [Requires pygtk]\n",
      "                     tkagg: yes [installing; run-time loading from Python Tcl /\n",
      "                            Tk]\n",
      "                     wxagg: no  [requires wxPython]\n",
      "                       gtk: no  [Requires pygtk]\n",
      "                       agg: yes [installing]\n",
      "                     cairo: no  [cairocffi or pycairo not found]\n",
      "                 windowing: no  [Microsoft Windows only]\n",
      "    \n",
      "    OPTIONAL LATEX DEPENDENCIES\n",
      "                    dvipng: no\n",
      "               ghostscript: no\n",
      "                     latex: no\n",
      "                   pdftops: no\n",
      "    \n",
      "    OPTIONAL PACKAGE DATA\n",
      "                      dlls: no  [skipping due to configuration]\n",
      "    \n",
      "    ============================================================================\n",
      "                            * The following required packages can not be built:\n",
      "                            * freetype * Try installing freetype with `brew\n",
      "                            * install freetype`\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/7_/qh7d98mx5xd7wx3gc4mxpwd80000gn/T/pip-install-ij3xp6n0/matplotlib/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (3.0.3)\n",
      "Requirement already satisfied: numpy in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (1.16.2)\n",
      "Requirement already satisfied: pandas in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (0.24.2)\n",
      "Collecting patsy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/0c/5f61f1a3d4385d6bf83b83ea495068857ff8dfb89e74824c6e9eb63286d8/patsy-0.5.1-py2.py3-none-any.whl (231kB)\n",
      "\u001b[K    100% |████████████████████████████████| 235kB 4.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scipy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/6c/ccf7403d14f0ab0f20ce611696921f204f4ffce99a4fd383c892a6a7e9eb/scipy-1.2.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (27.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 27.3MB 186kB/s ta 0:00:011  0% |▎                               | 204kB 4.1MB/s eta 0:00:07    3% |█                               | 860kB 25.8MB/s eta 0:00:02    80% |█████████████████████████▉      | 22.0MB 2.6MB/s eta 0:00:03    85% |███████████████████████████▎    | 23.3MB 13.2MB/s eta 0:00:01    89% |████████████████████████████▊   | 24.5MB 16.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting statsmodels\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/34/f32c5812145d80bdb5e92af73e2173d08012379d52a30e87bef5a8b1b6e1/statsmodels-0.9.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (9.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.6MB 775kB/s eta 0:00:01    39% |████████████▊                   | 3.8MB 27.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (4.31.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (from patsy) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /miniconda2/envs/deep-learning/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.0)\n",
      "Installing collected packages: patsy, scipy, statsmodels\n",
      "Successfully installed patsy-0.5.1 scipy-1.2.1 statsmodels-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib numpy pandas patsy scipy statsmodels tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import patsy\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statistics import median\n",
    "from scipy.stats import gaussian_kde\n",
    "from statsmodels.formula.api import ols\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We’ll be using the Barra dataset to get factors that can be used to predict risk. Loading and parsing the raw Barra data can be a very slow process that can significantly slow down your backtesting. For this reason, it's important to pre-process the data beforehand. For your convenience, the Barra data has already been pre-processed for you and saved into pickle files. You will load the Barra data from these pickle files.\n",
    "\n",
    "In the code below, we start by loading `2004` factor data from the `pandas-frames.2004.pickle` file. We also load the `2003` and `2004` covariance data from the `covaraince.2003.pickle`  and `covaraince.2004.pickle` files. You are encouraged  to customize the data range for your backtest. For example, we recommend starting with two or three years of factor data. Remember that the covariance data should include all the years that you choose for the factor data,   and also one year earlier. For example, in the code below we are using  `2004` factor data, therefore, we must include `2004` in our covariance data, but also the previous year, `2003`. If you don't remember why must include this previous year, feel free to review the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "barra_dir = 'barra_data/'\n",
    "\n",
    "data = {}\n",
    "for year in [2004]:\n",
    "    fil = barra_dir + \"pandas-frames.\" + str(year) + \".pickle\"\n",
    "    data.update(pickle.load( open( fil, \"rb\" ) ))\n",
    "    \n",
    "covariance = {}\n",
    "for year in [2004]:\n",
    "    fil = barra_dir + \"covariance.\" + str(year) + \".pickle\"\n",
    "    covariance.update(pickle.load( open(fil, \"rb\" ) ))\n",
    "    \n",
    "daily_return = {}\n",
    "for year in [2004, 2005]:\n",
    "    fil = barra_dir + \"price.\" + str(year) + \".pickle\"\n",
    "    daily_return.update(pickle.load( open(fil, \"rb\" ) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Daily Returns Data (TODO)\n",
    "\n",
    "In the cell below, we want to incorporate a realistic time delay that exists in live trading, we’ll use a two day delay for the `daily_return` data. That means the `daily_return` should be two days after the data in `data` and `cov_data`. Combine `daily_return` and `data` together in a dict called `frames`.\n",
    "\n",
    "Since reporting of PnL is usually for the date of the returns, make sure to use the two day delay dates (dates that match the `daily_return`) when building `frames`. This means calling `frames['20040108']` will get you the prices from \"20040108\" and the data from `data` at \"20040106\".\n",
    "\n",
    "Note: We're not shifting `covariance`, since we'll use the \"DataDate\" field in `frames` to lookup the covariance data. The \"DataDate\" field contains the date when the `data` in `frames` was recorded. For example, `frames['20040108']` will give you a value of \"20040106\" for the field \"DataDate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f5e1539bca26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         sorted(daily_return.keys())[dlyreturn_n_days_delay:len(data) + dlyreturn_n_days_delay])\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_date\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_shifts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprice_date\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdaily_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprice_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Barrid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'20040108'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   6866\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6867\u001b[0m                      \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6868\u001b[0;31m                      copy=copy, indicator=indicator, validate=validate)\n\u001b[0m\u001b[1;32m   6869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6870\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                          validate=validate)\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             concat_axis=0, copy=self.copy)\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2059\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m             b = make_block(\n\u001b[0;32m-> 2061\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                 placement=placement)\n\u001b[1;32m   2063\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    240\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m    241\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m--> 242\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    240\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m    241\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m--> 242\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 values = algos.take_nd(values, indexer, axis=ax,\n\u001b[0;32m--> 225\u001b[0;31m                                        fill_value=fill_value)\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda2/envs/deep-learning/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1653\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n\u001b[1;32m   1654\u001b[0m                                  mask_info=mask_info)\n\u001b[0;32m-> 1655\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frames ={}\n",
    "dlyreturn_n_days_delay = 2\n",
    "\n",
    "# TODO: Implement\n",
    "date_shifts = zip(\n",
    "        sorted(data.keys()),\n",
    "        sorted(daily_return.keys())[dlyreturn_n_days_delay:len(data) + dlyreturn_n_days_delay])\n",
    "for data_date, price_date in date_shifts:\n",
    "    frames[price_date] = data[data_date].merge(daily_return[price_date], on='Barrid')\n",
    "print(frames['20040108'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Daily Returns date column (Optional)\n",
    "Name the column `DlyReturnDate`.\n",
    "**Hint**: create a list containing copies of the date, then create a pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "for DlyReturnDate, df in daily_return.items():\n",
    "    n_rows= df.shape[0]\n",
    "    df['DlyReturnDate']= pd.Series([DlyReturnDate]*n_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winsorize\n",
    "\n",
    "As we have done in other projects, we'll want to avoid extremely positive or negative values in our data. Will therefore create a function, `wins`, that will clip our values to a minimum and maximum range. This process is called **Winsorizing**. Remember that this helps us handle noise, which may otherwise cause unusually large positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wins(x,a,b):\n",
    "    return np.where(x <= a,a, np.where(x >= b, b, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot\n",
    "\n",
    "Let's check our `wins` function by taking a look at the distribution of returns for a single day `20040102`. We will clip our data from `-0.1` to `0.1` and plot it using our `density_plot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot(data): \n",
    "    density = gaussian_kde(data)\n",
    "    xs = np.linspace(np.min(data),np.max(data),200)\n",
    "    density.covariance_factor = lambda : .25\n",
    "    density._compute_covariance()\n",
    "    plt.plot(xs,density(xs))\n",
    "    plt.xlabel('Daily Returns')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    \n",
    "test = frames['20040108']\n",
    "test['DlyReturn'] = wins(test['DlyReturn'],-0.1,0.1)\n",
    "density_plot(test['DlyReturn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Exposures and Factor Returns\n",
    "\n",
    "Recall that:\n",
    "\n",
    "$r_{i,t} = \\sum_{j=1}^{k} (\\beta_{i,j,t-2} \\times f_{j,t})$  \n",
    "where $i=1...N$ (N assets),   \n",
    "and $j=1...k$ (k factors).\n",
    "\n",
    "where $r_{i,t}$ is the return, $\\beta_{i,j,t-2}$ is the factor exposure, and $f_{j,t}$ is the factor return. Since we get the factor exposures from the Barra data, and we know the returns, it is possible to estimate the factor returns. In this notebook, we will use the Ordinary Least Squares (OLS) method to estimate the factor exposures, $f_{j,t}$, by using $\\beta_{i,j,t-2}$ as the independent variable, and $r_{i,t}$ as the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formula(factors, Y):\n",
    "    L = [\"0\"]\n",
    "    L.extend(factors)\n",
    "    return Y + \" ~ \" + \" + \".join(L)\n",
    "\n",
    "def factors_from_names(n):\n",
    "    return list(filter(lambda x: \"USFASTD_\" in x, n))\n",
    "\n",
    "def estimate_factor_returns(df): \n",
    "    ## build universe based on filters \n",
    "    estu = df.loc[df.IssuerMarketCap > 1e9].copy(deep=True)\n",
    "  \n",
    "    ## winsorize returns for fitting \n",
    "    estu['DlyReturn'] = wins(estu['DlyReturn'], -0.25, 0.25)\n",
    "  \n",
    "    all_factors = factors_from_names(list(df))\n",
    "    form = get_formula(all_factors, \"DlyReturn\")\n",
    "    model = ols(form, data=estu)\n",
    "    results = model.fit()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facret = {}\n",
    "\n",
    "for date in frames:\n",
    "    facret[date] = estimate_factor_returns(frames[date]).params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dates = sorted(list(map(lambda date: pd.to_datetime(date, format='%Y%m%d'), frames.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Alpha Factors\n",
    "\n",
    "We will now choose our alpha factors. Barra's factors include some alpha factors that we have seen before, such as:\n",
    "\n",
    "* **USFASTD_1DREVRSL** : Reversal\n",
    "\n",
    "* **USFASTD_EARNYILD** : Earnings Yield\n",
    "\n",
    "* **USFASTD_VALUE** : Value\n",
    "\n",
    "* **USFASTD_SENTMT** : Sentiment\n",
    "\n",
    "We will choose these alpha factors for now, but you are encouraged to come back to this later and try other factors as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_factors = [\"USFASTD_1DREVRSL\", \"USFASTD_EARNYILD\", \"USFASTD_VALUE\", \"USFASTD_SENTMT\"]\n",
    "\n",
    "facret_df = pd.DataFrame(index = my_dates)\n",
    "\n",
    "for dt in my_dates: \n",
    "    for alp in alpha_factors: \n",
    "        facret_df.at[dt, alp] = facret[dt.strftime('%Y%m%d')][alp]\n",
    "\n",
    "for column in facret_df.columns:\n",
    "        plt.plot(facret_df[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Factor Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Previous Portfolio Holdings \n",
    "\n",
    "In order to optimize our portfolio we will use the previous day's holdings to estimate the trade size and transaction costs. In order to keep track of the holdings from the previous day we will include a column to hold the portfolio holdings of the previous day. These holdings of all our assets will be initialized to zero when the backtest first starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nas(df): \n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    for numeric_column in numeric_columns: \n",
    "        df[numeric_column] = np.nan_to_num(df[numeric_column])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_holdings = pd.DataFrame(data = {\"Barrid\" : [\"USA02P1\"], \"h.opt.previous\" : np.array(0)})\n",
    "df = frames[my_dates[0].strftime('%Y%m%d')]\n",
    "\n",
    "df = df.merge(previous_holdings, how = 'left', on = 'Barrid')\n",
    "df = clean_nas(df)\n",
    "df.loc[df['SpecRisk'] == 0]['SpecRisk'] = median(df['SpecRisk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Universe Based on Filters (TODO)\n",
    "\n",
    "In the cell below, implement the function `get_universe` that creates a stock universe by selecting only those companies that have a market capitalization of at least 1 billion dollars **OR** that are in the previous day's holdings, even if on the current day, the company no longer meets the 1 billion dollar criteria.\n",
    "\n",
    "When creating the universe, make sure you use the `.copy()` attribute to create a copy of the data. Also, it is very important to make sure that we are not looking at returns when forming the portfolio! to make this impossible, make sure to drop the column containing the daily return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_universe(df):\n",
    "    \"\"\"\n",
    "    Create a stock universe based on filters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        All stocks\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    universe : DataFrame\n",
    "        Selected stocks based on filters\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    return df[(df['IssuerMarketCap'] >=1000000)|(df['h.opt.previous'] > 0)].copy()\n",
    "\n",
    "universe = get_universe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(int(universe['DataDate'][1])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors\n",
    "\n",
    "We will now extract both the risk factors and alpha factors. We begin by first getting all the factors using the `factors_from_names` function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = factors_from_names(list(universe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create the function `setdiff` to just select the factors that we have not defined as alpha factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setdiff(temp1, temp2): \n",
    "    s = set(temp2)\n",
    "    temp3 = [x for x in temp1 if x not in s]\n",
    "    return temp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factors = setdiff(all_factors, alpha_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also save the column that contains the previous holdings in a separate variable because we are going to use it later when we perform our portfolio optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = universe['h.opt.previous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix of Risk Factor Exposures\n",
    "\n",
    "Our dataframe contains several columns that we'll use as risk factors exposures.  Extract these and put them into a matrix.\n",
    "\n",
    "The data, such as industry category, are already one-hot encoded, but if this were not the case, then using `patsy.dmatrices` would help, as this function extracts categories and performs the one-hot encoding.  We'll practice using this package, as you may find it useful with future data sets.  You could also store the factors in a dataframe if you prefer.\n",
    "\n",
    "#### How to use patsy.dmatrices\n",
    "\n",
    "`patsy.dmatrices` takes in a formula and the dataframe.  The formula tells the function which columns to take.  The formula will look something like this:  \n",
    "`SpecRisk ~ 0 + USFASTD_AERODEF + USFASTD_AIRLINES + ...`  \n",
    "where the variable to the left of the ~ is the \"dependent variable\" and the others to the right are the independent variables (as if we were preparing data to be fit to a model).\n",
    "\n",
    "This just means that the `pasty.dmatrices` function will return two matrix variables, one that contains the single column for the dependent variable `outcome`, and the independent variable columns are stored in a matrix `predictors`.\n",
    "\n",
    "The `predictors` matrix will contain the matrix of risk factors, which is what we want.  We don't actually need the `outcome` matrix; it's just created because that's the way patsy.dmatrices works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = get_formula(risk_factors, \"SpecRisk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_matrix(formula, data): \n",
    "    outcome, predictors = patsy.dmatrices(formula, data)\n",
    "    return predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = model_matrix(formula, universe)\n",
    "BT = B.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Specific Variance\n",
    "\n",
    "Notice that the specific risk data is in percent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe['SpecRisk'][0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, in order to get the specific variance for each stock in the universe we first need to multiply these values by `0.01`  and then square them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specVar = (0.01 * universe['SpecRisk']) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor covariance matrix (TODO)\n",
    "\n",
    "Note that we already have factor covariances from Barra data, which is stored in the variable `covariance`.  `covariance` is a dictionary, where the key is each day's date, and the value is a dataframe containing the factor covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance['20040102'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, implement the function `diagonal_factor_cov` to create the factor covariance matrix. Note that the covariances are given in percentage units squared.  Therefore you must re-scale them appropriately so that they're in decimals squared. Use the given `colnames` function to get the column names from `B`. \n",
    "\n",
    "When creating factor covariance matrix, you can store the factor variances and covariances, or just store the factor variances.  Try both, and see if you notice any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colnames(B):\n",
    "    if type(B) == patsy.design_info.DesignMatrix: \n",
    "        return B.design_info.column_names\n",
    "    if type(B) == pandas.core.frame.DataFrame: \n",
    "        return B.columns.tolist()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_factor_cov(date, B):\n",
    "    \"\"\"\n",
    "    Create the factor covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date : string\n",
    "           date. For example 20040102\n",
    "        \n",
    "    B : patsy.design_info.DesignMatrix OR pandas.core.frame.DataFrame\n",
    "        Matrix of Risk Factors\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Fm : Numpy ndarray\n",
    "        factor covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    cv = covariance[date]\n",
    "    k = np.shape(B)[1]\n",
    "    Fm = np.zeros([k,k])\n",
    "    for j in range(0,k): \n",
    "        factor = colnames(B)[j]\n",
    "        Fm[j,j] = (0.01**2) * get_variance(cv, factor)\n",
    "    return(Fm)\n",
    "\n",
    "Fvar = diagonal_factor_cov(date, B)\n",
    "print(Fvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Costs\n",
    "\n",
    "To get the transaction cost, or slippage, we have to multiply the price change due to market impact by the amount of dollars traded:\n",
    "\n",
    "$$\n",
    "\\mbox{tcost_{i,t}} = \\% \\Delta \\mbox{price}_{i,t} \\times \\mbox{trade}_{i,t}\n",
    "$$\n",
    "\n",
    "In summation notation it looks like this:  \n",
    "$$\n",
    "\\mbox{tcost}_{i,t} = \\sum_i^{N} \\lambda_{i,t} (h_{i,t} - h_{i,t-1})^2\n",
    "$$  \n",
    "where\n",
    "$$\n",
    "\\lambda_{i,t} = \\frac{1}{10\\times \\mbox{ADV}_{i,t}}\n",
    "$$\n",
    "\n",
    "Note that since we're dividing by ADV, we'll want to handle cases when ADV is missing or zero.  In those instances, we can set ADV to a small positive number, such as 10,000, which, in practice assumes that the stock is illiquid. In the code below if there is no volume information we assume the asset is illiquid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda(universe, composite_volume_column = 'ADTCA_30'):\n",
    "    universe.loc[np.isnan(universe[composite_volume_column]), composite_volume_column] = 1.0e4\n",
    "    universe.loc[universe[composite_volume_column] == 0, composite_volume_column] = 1.0e4 \n",
    "\n",
    "    adv = universe[composite_volume_column]\n",
    "    \n",
    "    return 0.1 / adv\n",
    "\n",
    "Lambda = get_lambda(universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Combination (TODO)\n",
    "\n",
    "In the code below create a matrix of alpha factors and return it from the function `get_B_alpha`. Create this matrix in the same way you created the matrix of risk factors, i.e. using the `get_formula` and `model_matrix` functions we have defined above. Feel free to go back and look at the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_B_alpha(alpha_factors, universe):\n",
    "    # TODO: Implement\n",
    "    \n",
    "    return model_matrix(get_formula(alpha_factors, \"SpecRisk\"), data = universe)\n",
    "\n",
    "B_alpha = get_B_alpha(alpha_factors, universe)\n",
    "print((B_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the matrix containing the alpha factors we will combine them by adding its rows. By doing this we will collapse the `B_alpha` matrix into a single alpha vector. We'll multiply by `1e-4` so that the expression of expected portfolio return, $\\alpha^T \\mathbf{h}$, is in dollar units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_vec(B_alpha):\n",
    "    \"\"\"\n",
    "    Create an alpha vecrtor\n",
    "\n",
    "    Parameters\n",
    "    ----------        \n",
    "    B_alpha : patsy.design_info.DesignMatrix \n",
    "        Matrix of Alpha Factors\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    alpha_vec : patsy.design_info.DesignMatrix \n",
    "        alpha vecrtor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    scale = 1e-4\n",
    "    alpha_vector = scale * np.sum(B_alpha, axis=1)\n",
    "    return alpha_vector\n",
    "\n",
    "alpha_vec = get_alpha_vec(B_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Challenge\n",
    "\n",
    "You can also try to a more sophisticated method of alpha combination, by choosing the holding for each alpha based on the same metric of its performance, such as the factor returns, or sharpe ratio.  To make this more realistic, you can calculate a rolling average of the sharpe ratio, which is updated for each day.  Remember to only use data that occurs prior to the date of each optimization, and not data that occurs in the future.  Also, since factor returns and sharpe ratios may be negative, consider using a `max` function to give the holdings a lower bound of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function (TODO)\n",
    "\n",
    "The objective function is given by:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{h}) = \\frac{1}{2}\\kappa \\mathbf{h}_t^T\\mathbf{Q}^T\\mathbf{Q}\\mathbf{h}_t + \\frac{1}{2} \\kappa \\mathbf{h}_t^T \\mathbf{S} \\mathbf{h}_t - \\mathbf{\\alpha}^T \\mathbf{h}_t + (\\mathbf{h}_{t} - \\mathbf{h}_{t-1})^T \\mathbf{\\Lambda} (\\mathbf{h}_{t} - \\mathbf{h}_{t-1})\n",
    "$$\n",
    "\n",
    "Where the terms correspond to: factor risk + idiosyncratic risk - expected portfolio return + transaction costs, respectively. We should also note that $\\textbf{Q}^T\\textbf{Q}$ is defined to be the same as $\\textbf{BFB}^T$.  Review the lessons if you need a refresher of how we get $\\textbf{Q}$.\n",
    "\n",
    "Our objective is to minimize this objective function. To do this, we will use Scipy's optimization function:\n",
    "\n",
    "`scipy.optimize.fmin_l_bfgs_b(func, initial_guess, func_gradient)`\n",
    "\n",
    "where:\n",
    "\n",
    "* **func** : is the function we want to minimize\n",
    "\n",
    "* **initial_guess** : is out initial guess\n",
    "\n",
    "* **func_gradient** : is the gradient of the function we want to minimize\n",
    "\n",
    "So, in order to use the `scipy.optimize.fmin_l_bfgs_b` function we first need to define its parameters.\n",
    "\n",
    "In the code below implement the function `obj_func(h)` that corresponds to the objective function above that we want to minimize. We will set the risk aversion to be `1.0e-6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_aversion = 1.0e-6\n",
    "\n",
    "def get_obj_func(h0, risk_aversion, Q, specVar, alpha_vec, Lambda): \n",
    "    def obj_func(h):\n",
    "        # TODO: Implement\n",
    "        return 0.5*risk_aversion*np.sum(np.matmul(Q, h)**2) + 0.5*risk_aversion*np.dot(h**2, specVar) - np.dot(h, alpha_vec) + np.dot((h - h0)**2, Lambda)\n",
    "    \n",
    "    return obj_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient (TODO)\n",
    "\n",
    "Now that we can generate the objective function using `get_obj_func`, we can now create a similar function with its gradient. The reason we're interested in calculating the gradient is so that we can tell the optimizer in which direction, and how much, it should shift the portfolio holdings in order to improve the objective function (minimize variance, minimize transaction cost, and maximize expected portfolio return).\n",
    "\n",
    "Before we implement the function we first need to know what the gradient looks like. The gradient, or derivative of the objective function, with respect to the portfolio holdings h, is given by:  \n",
    "\n",
    "$$\n",
    "f'(\\mathbf{h}) = \\frac{1}{2}\\kappa (2\\mathbf{Q}^T\\mathbf{Qh}) + \\frac{1}{2}\\kappa (2\\mathbf{Sh}) - \\mathbf{\\alpha} + 2(\\mathbf{h}_{t} - \\mathbf{h}_{t-1}) \\mathbf{\\Lambda}\n",
    "$$\n",
    "\n",
    "In the code below, implement the function `grad(h)` that corresponds to the function of the gradient given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_func(h0, risk_aversion, Q, QT, specVar, alpha_vec, Lambda):\n",
    "    def grad_func(h):\n",
    "        # TODO: Implement\n",
    "        gradient = risk_aversion*(np.matmul(QT, np.matmul(Q,h)) + (specVar*h) ) - alpha_vec + 2*(h-h0)*Lambda\n",
    "        return np.asarray(gradient)\n",
    "    \n",
    "    return grad_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize (TODO)\n",
    "\n",
    "Now that we can generate the objective function using `get_obj_func`, and its corresponding gradient using `get_grad_func` we are ready to minimize the objective function using Scipy's optimization function. For this, we will use out initial holdings as our `initial_guess` parameter.\n",
    "\n",
    "In the cell below, implement the function `get_h_star` that optimizes the objective function. Use the objective function (`obj_func`) and gradient function (`grad_func`) provided within `get_h_star` to optimize the objective function using the `scipy.optimize.fmin_l_bfgs_b` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_aversion = 1.0e-6\n",
    "\n",
    "Q = np.matmul(scipy.linalg.sqrtm(Fvar), BT)\n",
    "QT = Q.transpose()\n",
    "\n",
    "def get_h_star(risk_aversion, Q, QT, specVar, alpha_vec, h0, Lambda):\n",
    "    \"\"\"\n",
    "    Optimize the objective function\n",
    "\n",
    "    Parameters\n",
    "    ----------        \n",
    "    risk_aversion : int or float \n",
    "        Trader's risk aversion\n",
    "        \n",
    "    Q : patsy.design_info.DesignMatrix \n",
    "        Q Matrix\n",
    "        \n",
    "    QT : patsy.design_info.DesignMatrix \n",
    "        Transpose of the Q Matrix\n",
    "        \n",
    "    specVar: Pandas Series \n",
    "        Specific Variance\n",
    "        \n",
    "    alpha_vec: patsy.design_info.DesignMatrix \n",
    "        alpha vector\n",
    "        \n",
    "    h0 : Pandas Series  \n",
    "        initial holdings\n",
    "        \n",
    "    Lambda : Pandas Series  \n",
    "        Lambda\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    optimizer_result[0]: Numpy ndarray \n",
    "        optimized holdings\n",
    "    \"\"\"\n",
    "    obj_func = get_obj_func(h0, risk_aversion, Q, specVar, alpha_vec, Lambda)\n",
    "    grad_func = get_grad_func(h0, risk_aversion, Q, QT, specVar, alpha_vec, Lambda)\n",
    "    \n",
    "    # TODO: Implement \n",
    "    result=scipy.optimize.fmin_l_bfgs_b(obj_func, h0, grad_func)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have optimized our objective function we can now use, `h_star` to create our optimal portfolio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_portfolio = pd.DataFrame(data = {\"Barrid\" : universe['Barrid'], \"h.opt\" : h_star})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Exposures (TODO)\n",
    "\n",
    "We can also use `h_star` to calculate our portfolio's risk and alpha exposures.\n",
    "\n",
    "In the cells below implement the functions `get_risk_exposures` and `get_portfolio_alpha_exposure` that calculate the portfolio's risk and alpha exposures, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_risk_exposures(B, BT, h_star):\n",
    "    \"\"\"\n",
    "    Calculate portfolio's Risk Exposure\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    B : patsy.design_info.DesignMatrix \n",
    "        Matrix of Risk Factors\n",
    "        \n",
    "    BT : patsy.design_info.DesignMatrix \n",
    "        Transpose of Matrix of Risk Factors\n",
    "        \n",
    "    h_star: Numpy ndarray \n",
    "        optimized holdings\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    risk_exposures : Pandas Series\n",
    "        Risk Exposures\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    risk_exposures = np.matmul(BT, h_star)\n",
    "    return pd.Series(risk_exposures ,index=colnames(B))\n",
    "\n",
    "risk_exposures = get_risk_exposures(B, BT, h_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio_alpha_exposure(B_alpha, h_star):\n",
    "    \"\"\"\n",
    "    Calculate portfolio's Alpha Exposure\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    B_alpha : patsy.design_info.DesignMatrix \n",
    "        Matrix of Alpha Factors\n",
    "        \n",
    "    h_star: Numpy ndarray \n",
    "        optimized holdings\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    alpha_exposures : Pandas Series\n",
    "        Alpha Exposures\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    alpha_exposures = np.matmul(B_alpha.transpose(), h_star)\n",
    "    return pd.Series(alpha_exposures ,index=colnames(B_alpha))\n",
    "\n",
    "portfolio_alpha_exposure = get_portfolio_alpha_exposure(B_alpha, h_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Costs (TODO)\n",
    "\n",
    "We can also use `h_star` to calculate our total transaction costs:\n",
    "$$\n",
    "\\mbox{tcost} = \\sum_i^{N} \\lambda_{i} (h_{i,t} - h_{i,t-1})^2\n",
    "$$\n",
    "\n",
    "In the cell below, implement the function `get_total_transaction_costs` that calculates the total transaction costs according to the equation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_transaction_costs(h0, h_star, Lambda):\n",
    "    \"\"\"\n",
    "    Calculate Total Transaction Costs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h0 : Pandas Series\n",
    "        initial holdings (before optimization)\n",
    "        \n",
    "    h_star: Numpy ndarray \n",
    "        optimized holdings\n",
    "        \n",
    "    Lambda : Pandas Series  \n",
    "        Lambda\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    total_transaction_costs : float\n",
    "        Total Transaction Costs\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    return np.sum(Lambda*np.square(h_star-h0.values))\n",
    "\n",
    "total_transaction_costs = get_total_transaction_costs(h0, h_star, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "We can now take all the above functions we created above and use them to create a single function, `form_optimal_portfolio` that returns the optimal portfolio, the risk and alpha exposures, and the total transactions costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_optimal_portfolio(df, previous, risk_aversion):\n",
    "    df = df.merge(previous, how = 'left', on = 'Barrid')\n",
    "    df = clean_nas(df)\n",
    "    df.loc[df['SpecRisk'] == 0]['SpecRisk'] = median(df['SpecRisk'])\n",
    "  \n",
    "    universe = get_universe(df)\n",
    "    date = str(int(universe['DataDate'][1]))\n",
    "  \n",
    "    all_factors = factors_from_names(list(universe))\n",
    "    risk_factors = setdiff(all_factors, alpha_factors)\n",
    "  \n",
    "    h0 = universe['h.opt.previous']\n",
    "  \n",
    "    B = model_matrix(get_formula(risk_factors, \"SpecRisk\"), universe)\n",
    "    BT = B.transpose()\n",
    "  \n",
    "    specVar = (0.01 * universe['SpecRisk']) ** 2\n",
    "    Fvar = diagonal_factor_cov(date, B)\n",
    "    \n",
    "    Lambda = get_lambda(universe)\n",
    "    B_alpha = get_B_alpha(alpha_factors, universe)\n",
    "    alpha_vec = get_alpha_vec(B_alpha)\n",
    "  \n",
    "    Q = np.matmul(scipy.linalg.sqrtm(Fvar), BT)\n",
    "    QT = Q.transpose()\n",
    "    \n",
    "    h_star = get_h_star(risk_aversion, Q, QT, specVar, alpha_vec, h0, Lambda)\n",
    "    opt_portfolio = pd.DataFrame(data = {\"Barrid\" : universe['Barrid'], \"h.opt\" : h_star})\n",
    "    \n",
    "    risk_exposures = get_risk_exposures(B, BT, h_star)\n",
    "    portfolio_alpha_exposure = get_portfolio_alpha_exposure(B_alpha, h_star)\n",
    "    total_transaction_costs = get_total_transaction_costs(h0, h_star, Lambda)\n",
    "  \n",
    "    return {\n",
    "        \"opt.portfolio\" : opt_portfolio, \n",
    "        \"risk.exposures\" : risk_exposures, \n",
    "        \"alpha.exposures\" : portfolio_alpha_exposure,\n",
    "        \"total.cost\" : total_transaction_costs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tradelist\n",
    "\n",
    "The trade list is the most recent optimal asset holdings minus the previous day's optimal holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tradelist(prev_holdings, opt_result):\n",
    "    tmp = prev_holdings.merge(opt_result['opt.portfolio'], how='outer', on = 'Barrid')\n",
    "    tmp['h.opt.previous'] = np.nan_to_num(tmp['h.opt.previous'])\n",
    "    tmp['h.opt'] = np.nan_to_num(tmp['h.opt'])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save optimal holdings as previous optimal holdings.\n",
    "\n",
    "As we walk through each day, we'll re-use the column for previous holdings by storing the \"current\" optimal holdings as the \"previous\" optimal holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_previous(result): \n",
    "    prev = result['opt.portfolio']\n",
    "    prev = prev.rename(index=str, columns={\"h.opt\": \"h.opt.previous\"}, copy=True, inplace=False)\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the backtest\n",
    "\n",
    "Walk through each day, calculating the optimal portfolio holdings and trade list.  This may take some time, but should finish sooner if you've chosen all the optimizations you learned in the lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trades = {}\n",
    "port = {}\n",
    "\n",
    "for dt in tqdm(my_dates, desc='Optimizing Portfolio', unit='day'):\n",
    "    date = dt.strftime('%Y%m%d')\n",
    "\n",
    "    result = form_optimal_portfolio(frames[date], previous_holdings, risk_aversion)\n",
    "    trades[date] = build_tradelist(previous_holdings, result)\n",
    "    port[date] = result\n",
    "    previous_holdings = convert_to_previous(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profit-and-Loss (PnL) attribution (TODO)\n",
    "\n",
    "Profit and Loss is the aggregate realized daily returns of the assets, weighted by the optimal portfolio holdings chosen, and summed up to get the portfolio's profit and loss.\n",
    "\n",
    "The PnL attributed to the alpha factors equals the factor returns times factor exposures for the alpha factors.  \n",
    "\n",
    "$$\n",
    "\\mbox{PnL}_{alpha}= f \\times b_{alpha}\n",
    "$$\n",
    "\n",
    "Similarly, the PnL attributed to the risk factors equals the factor returns times factor exposures of the risk factors.\n",
    "\n",
    "$$\n",
    "\\mbox{PnL}_{risk} = f \\times b_{risk}\n",
    "$$\n",
    "\n",
    "In the code below, in the function `build_pnl_attribution` calculate the PnL attributed to the alpha factors, the PnL attributed to the risk factors, and attribution to cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assumes v, w are pandas Series \n",
    "def partial_dot_product(v, w):\n",
    "    common = v.index.intersection(w.index)\n",
    "    return np.sum(v[common] * w[common])\n",
    "\n",
    "def build_pnl_attribution(): \n",
    "\n",
    "    df = pd.DataFrame(index = my_dates)\n",
    "    \n",
    "    for dt in my_dates:\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "\n",
    "        p = port[date]\n",
    "        fr = facret[date]\n",
    "\n",
    "        mf = p['opt.portfolio'].merge(frames[date], how = 'left', on = \"Barrid\")\n",
    "        \n",
    "        mf['DlyReturn'] = wins(mf['DlyReturn'], -0.5, 0.5)\n",
    "        df.at[dt,\"daily.pnl\"] = np.sum(mf['h.opt'] * mf['DlyReturn'])\n",
    "        \n",
    "        # TODO: Implement\n",
    "    \n",
    "        df.at[dt,\"attribution.alpha.pnl\"] = partial_dot_product(fr,p['alpha.exposures'])\n",
    "        df.at[dt,\"attribution.risk.pnl\"] = partial_dot_product(fr,p['risk.exposures'])\n",
    "        df.at[dt,\"attribution.cost\"] = p['total.cost']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = build_pnl_attribution()\n",
    "\n",
    "for column in attr.columns:\n",
    "        plt.plot(attr[column].cumsum(), label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('PnL Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build portfolio characteristics (TODO)\n",
    "Calculate the sum of long positions, short positions, net positions, gross market value, and amount of dollars traded.\n",
    "\n",
    "In the code below, in the function `build_portfolio_characteristics` calculate the sum of long positions, short positions, net positions, gross market value, and amount of dollars traded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_portfolio_characteristics(): \n",
    "    df = pd.DataFrame(index = my_dates)\n",
    "    \n",
    "    for dt in my_dates:\n",
    "        date = dt.strftime('%Y%m%d')\n",
    "  \n",
    "        p = port[date]\n",
    "        tradelist = trades[date]\n",
    "        h = p['opt.portfolio']['h.opt']\n",
    "        \n",
    "        # TODO: Implement\n",
    "        \n",
    "        df.at[dt,\"long\"] = np.sum(h[h>0])\n",
    "        df.at[dt,\"short\"] = np.sum(h[h<0])\n",
    "        df.at[dt,\"net\"] = df.at[dt,\"long\"] + df.at[dt,\"short\"]\n",
    "        df.at[dt,\"gmv\"] = np.sum(abs(df.at[dt,\"long\"]) + abs(df.at[dt,\"short\"]))\n",
    "        df.at[dt,\"traded\"] = np.sum(np.abs(tradelist['h.opt.previous'] - tradelist['h.opt']))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pchar = build_portfolio_characteristics()\n",
    "\n",
    "for column in pchar.columns:\n",
    "        plt.plot(pchar[column], label=column)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Portfolio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional\n",
    "Choose additional metrics to evaluate your portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
